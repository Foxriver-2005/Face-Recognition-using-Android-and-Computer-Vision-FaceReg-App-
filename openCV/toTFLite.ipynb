{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c33a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 12:49:08.648771: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-22 12:49:08.973647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 12:49:08.973738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 12:49:09.036486: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-22 12:49:09.164253: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-22 12:49:09.165854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 12:49:10.742982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Global batch size: 512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Model, mixed_precision\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, DepthwiseConv2D, ZeroPadding2D, BatchNormalization, ReLU, Add, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# mixed precision training is used\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# 128 for 3060M, 512 for P100 \n",
    "GLOBAL_BATCH_SIZE = 512\n",
    "print('Global batch size: {}'.format(GLOBAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88da77cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: faces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(label, n_classes)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (img, label), label\n\u001b[0;32m---> 19\u001b[0m file_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFRecordDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(file_dataset,num_parallel_reads\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     21\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(parse_function,num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:1301\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[1;32m   1294\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1295\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1297\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1299\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1301\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[1;32m   1304\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[1;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: faces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord'"
     ]
    }
   ],
   "source": [
    "def parse_function(example_proto, n_classes=85742):\n",
    "    features = {'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "                'label': tf.io.FixedLenFeature([], tf.int64)}\n",
    "    features = tf.io.parse_single_example(example_proto, features)\n",
    "    img = tf.image.decode_jpeg(features['image_raw'])\n",
    "    img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "    # You can do more image distortion here for training data\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    #img = tf.subtract(img, 127.5)\n",
    "    img = tf.subtract(img, 128)\n",
    "    img = tf.multiply(img,  0.0078125)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    label = tf.cast(features['label'], tf.int64)\n",
    "    label = tf.one_hot(label, n_classes)\n",
    "    return (img, label), label\n",
    "\n",
    "\n",
    "file_dataset = tf.data.TFRecordDataset.list_files(\"faces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord\")\n",
    "train_dataset = tf.data.TFRecordDataset(file_dataset,num_parallel_reads=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27c1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model\n",
    "class ArcFace(Layer):\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "\n",
    "        embedding, labels = inputs\n",
    "\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.divide(embedding, embedding_norm, name='norm_embedding')\n",
    "        weights = self.W\n",
    "        weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "        weights = tf.divide(weights, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        mask = labels\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "\n",
    "        logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "        out = tf.nn.softmax(logit)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'s': self.s, 'm': self.m, 'n_classes': self.n_classes}\n",
    "\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 2 if tf.keras.backend.image_data_format() == 'channels_first' else 1\n",
    "    input_size = tf.keras.backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "\n",
    "def inverted_res_block(inputs, expansion, stride, filters, block_id):\n",
    "    channel_axis = -1\n",
    "    in_channels = tf.keras.backend.int_shape(inputs)[channel_axis]\n",
    "    pointwise_filters = filters\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "\n",
    "    # Expand\n",
    "    x = Conv2D(\n",
    "        expansion * in_channels,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'expand')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'expand_BN')(x)\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = ZeroPadding2D(\n",
    "            padding=correct_pad(x, 3),\n",
    "            name=prefix + 'pad')(x)\n",
    "    x = DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        strides=stride,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same' if stride == 1 else 'valid',\n",
    "        name=prefix + 'depthwise')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'depthwise_BN')(x)\n",
    "\n",
    "    x = ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    # Project\n",
    "    x = Conv2D(\n",
    "        pointwise_filters,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'project')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'project_BN')(x)\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return Add(name=prefix + 'add')([inputs, x])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def mobilefacenet_arcface(n_classes=85742):\n",
    "    weight_decay=0.00005\n",
    "    \n",
    "    # input\n",
    "    input = Input(shape=(112, 112, 3), name='input')\n",
    "    y = Input(shape=(n_classes), name='label')\n",
    "\n",
    "    # Conv2D\n",
    "    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec1_conv2d')(input)\n",
    "    x = BatchNormalization(name='sec1_bn')(x)\n",
    "    x = ReLU(name='sec1_relu')(x)\n",
    "\n",
    "    # DepthwiseConv2D\n",
    "    x = DepthwiseConv2D((3,3), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec2_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec2_bn')(x)\n",
    "    x = ReLU(name='sec2_relu')(x)\n",
    "    x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec2_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec2_bn2')(x)\n",
    "\n",
    "    # InvertedResidualBlock\n",
    "    x = inverted_res_block(x, 2, 2,  64, 0)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 1)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 2)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 3)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 4)\n",
    "\n",
    "    # Conv2D 1x1\n",
    "    x = Conv2D(512, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec8_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec8_bn')(x)\n",
    "    x = ReLU(name='sec8_relu')(x)\n",
    "\n",
    "    # linear GDConv 7x7\n",
    "    x = DepthwiseConv2D((7,7), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec9_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec9_bn')(x)\n",
    "    x = Conv2D(512, (1, 1), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec9_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec9_bn2')(x)\n",
    "\n",
    "    # linear Conv2D 1x1\n",
    "    x = Conv2D(128, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec10_conv2d')(x)\n",
    "    x = BatchNormalization(name='sec10_bn')(x)\n",
    "\n",
    "    # faltten\n",
    "    x = Flatten(dtype='float32')(x)\n",
    "\n",
    "    # embedding\n",
    "    x = tf.keras.layers.Lambda(lambda k: tf.keras.backend.l2_normalize(k, axis=-1), name=\"embedding\")(x)\n",
    "\n",
    "    # loss function\n",
    "    output = ArcFace(n_classes=85742, dtype='float32')([x, y])\n",
    "\n",
    "    return Model([input, y], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:09:56.613246: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 425852928 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = mobilefacenet_arcface()\n",
    "opt = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#model.summary()\n",
    "checkpoint_filepath = './ckpt/epoch_{epoch:02d}'\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return 0.01\n",
    "    elif epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=False)\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq=\"epoch\")\n",
    "\n",
    "model.fit(train_dataset,\n",
    "    epochs=7,\n",
    "    callbacks=[checkpoint_cb, tensorboard_cb, lr_cb],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the global policy to fp32 and rebuild a fp32 model, this is required for exporting to TFLite model\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "f32_model = mobilefacenet_arcface()\n",
    "f32_model.set_weights(model.get_weights())\n",
    "\n",
    "# extract the output model from the model used for training\n",
    "outputModel = tf.keras.Model(f32_model.get_layer('input').input, f32_model.get_layer('embedding').output, trainable=False)\n",
    "\n",
    "# save the trained model as TensorFlow SavedModel format\n",
    "outputModel.save('/output_model')\n",
    "\n",
    "# Convert the output model into a TensorflowLite model and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(outputModel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('output_model/MobileFaceNet.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
