{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9565ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Model, mixed_precision\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, DepthwiseConv2D, ZeroPadding2D, BatchNormalization, ReLU, Add, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# mixed precision training is used\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# 128 for 3060M, 512 for P100 \n",
    "GLOBAL_BATCH_SIZE = 512\n",
    "print('Global batch size: {}'.format(GLOBAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90577c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(example_proto, n_classes=85742):\n",
    "    features = {'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "                'label': tf.io.FixedLenFeature([], tf.int64)}\n",
    "    features = tf.io.parse_single_example(example_proto, features)\n",
    "    img = tf.image.decode_jpeg(features['image_raw'])\n",
    "    img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "    # You can do more image distortion here for training data\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    #img = tf.subtract(img, 127.5)\n",
    "    img = tf.subtract(img, 128)\n",
    "    img = tf.multiply(img,  0.0078125)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    label = tf.cast(features['label'], tf.int64)\n",
    "    label = tf.one_hot(label, n_classes)\n",
    "    return (img, label), label\n",
    "\n",
    "\n",
    "file_dataset = tf.data.TFRecordDataset.list_files(\"/kaggle/input/faces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord\")\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(file_dataset,num_parallel_reads=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model\n",
    "class ArcFace(Layer):\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "\n",
    "        embedding, labels = inputs\n",
    "\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.divide(embedding, embedding_norm, name='norm_embedding')\n",
    "        weights = self.W\n",
    "        weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "        weights = tf.divide(weights, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        mask = labels\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "\n",
    "        logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "        out = tf.nn.softmax(logit)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'s': self.s, 'm': self.m, 'n_classes': self.n_classes}\n",
    "\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 2 if tf.keras.backend.image_data_format() == 'channels_first' else 1\n",
    "    input_size = tf.keras.backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "\n",
    "def inverted_res_block(inputs, expansion, stride, filters, block_id):\n",
    "    channel_axis = -1\n",
    "    in_channels = tf.keras.backend.int_shape(inputs)[channel_axis]\n",
    "    pointwise_filters = filters\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "\n",
    "    # Expand\n",
    "    x = Conv2D(\n",
    "        expansion * in_channels,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'expand')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'expand_BN')(x)\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = ZeroPadding2D(\n",
    "            padding=correct_pad(x, 3),\n",
    "            name=prefix + 'pad')(x)\n",
    "    x = DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        strides=stride,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same' if stride == 1 else 'valid',\n",
    "        name=prefix + 'depthwise')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'depthwise_BN')(x)\n",
    "\n",
    "    x = ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    # Project\n",
    "    x = Conv2D(\n",
    "        pointwise_filters,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'project')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'project_BN')(x)\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return Add(name=prefix + 'add')([inputs, x])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def mobilefacenet_arcface(n_classes=85742):\n",
    "    weight_decay=0.00005\n",
    "    \n",
    "    # input\n",
    "    input = Input(shape=(112, 112, 3), name='input')\n",
    "    y = Input(shape=(n_classes), name='label')\n",
    "\n",
    "    # Conv2D\n",
    "    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec1_conv2d')(input)\n",
    "    x = BatchNormalization(name='sec1_bn')(x)\n",
    "    x = ReLU(name='sec1_relu')(x)\n",
    "\n",
    "    # DepthwiseConv2D\n",
    "    x = DepthwiseConv2D((3,3), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec2_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec2_bn')(x)\n",
    "    x = ReLU(name='sec2_relu')(x)\n",
    "    x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec2_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec2_bn2')(x)\n",
    "\n",
    "    # InvertedResidualBlock\n",
    "    x = inverted_res_block(x, 2, 2,  64, 0)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 1)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 2)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 3)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 4)\n",
    "\n",
    "    # Conv2D 1x1\n",
    "    x = Conv2D(512, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec8_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec8_bn')(x)\n",
    "    x = ReLU(name='sec8_relu')(x)\n",
    "\n",
    "    # linear GDConv 7x7\n",
    "    x = DepthwiseConv2D((7,7), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec9_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec9_bn')(x)\n",
    "    x = Conv2D(512, (1, 1), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec9_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec9_bn2')(x)\n",
    "\n",
    "    # linear Conv2D 1x1\n",
    "    x = Conv2D(128, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec10_conv2d')(x)\n",
    "    x = BatchNormalization(name='sec10_bn')(x)\n",
    "\n",
    "    # faltten\n",
    "    x = Flatten(dtype='float32')(x)\n",
    "\n",
    "    # embedding\n",
    "    x = tf.keras.layers.Lambda(lambda k: tf.keras.backend.l2_normalize(k, axis=-1), name=\"embedding\")(x)\n",
    "\n",
    "    # loss function\n",
    "    output = ArcFace(n_classes=85742, dtype='float32')([x, y])\n",
    "\n",
    "    return Model([input, y], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ddbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilefacenet_arcface()\n",
    "opt = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#model.summary()\n",
    "checkpoint_filepath = './ckpt/epoch_{epoch:02d}'\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return 0.01\n",
    "    elif epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=False)\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq=\"epoch\")\n",
    "\n",
    "model.fit(train_dataset,\n",
    "    epochs=7,\n",
    "    callbacks=[checkpoint_cb, tensorboard_cb, lr_cb],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the global policy to fp32 and rebuild a fp32 model, this is required for exporting to TFLite model\n",
    "# see issue at https://github.com/tensorflow/tensorflow/issues/46380\n",
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "f32_model = mobilefacenet_arcface()\n",
    "f32_model.set_weights(model.get_weights())\n",
    "\n",
    "# extract the output model from the model used for training\n",
    "outputModel = tf.keras.Model(f32_model.get_layer('input').input, f32_model.get_layer('embedding').output, trainable=False)\n",
    "\n",
    "# save the trained model as TensorFlow SavedModel format\n",
    "outputModel.save('/kaggle/working/output_model')\n",
    "\n",
    "# Convert the output model into a TensorflowLite model and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(outputModel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('/kaggle/working/output_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1da47d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[1;32m      2\u001b[0m converter \u001b[38;5;241m=\u001b[39m lite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacenet_keras.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tfmodel \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import lite\n",
    "converter = lite.TFLiteConverter.from_keras_model_file('facenet_keras.h5')\n",
    "tfmodel = converter.convert()\n",
    "open('MobileFaceNet.tflite','wb').write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af7b5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7fe066772e90>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacenet_keras_weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m converter \u001b[38;5;241m=\u001b[39m lite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[1;32m      6\u001b[0m tfmodel \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/legacy/hdf5_format.py:197\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    195\u001b[0m model_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model config found in the file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7fe066772e90>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import lite\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('facenet_keras_weights.h5')\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tfmodel = converter.convert()\n",
    "open('MobileFaceNet.tflite','wb').write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40dac51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_json\n\u001b[1;32m      4\u001b[0m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_from_json(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacenet_keras.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "keras.backend.clear_session()\n",
    "with open('model.json','r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "model.load_weights('facenet_keras.h5')\n",
    "model.summary()\n",
    "model.save('model_full.h5')\n",
    "model = tf.keras.models.load_model('model_full.h5',compile=False)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with tf.io.gfile.GFile('MobileFaceNet.tflite','wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94054fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 12:13:51.601015: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-01 12:13:51.671479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 12:13:51.671513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 12:13:51.672573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 12:13:51.678883: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-01 12:13:51.679329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 12:13:53.082112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.lite' has no attribute 'TocoConverter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacenet.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Converts the Keras model to TensorFlow Lite\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m converter \u001b[38;5;241m=\u001b[39m \u001b[43mlite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTocoConverter\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_keras_model_file(input_file)\n\u001b[1;32m      8\u001b[0m converter\u001b[38;5;241m.\u001b[39mpost_training_quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.lite' has no attribute 'TocoConverter'"
     ]
    }
   ],
   "source": [
    "import tensorflow.lite as lite\n",
    "\n",
    "input_file = \"facenet_keras_weights.h5\"\n",
    "output_file = \"facenet.tflite\"\n",
    "\n",
    "# Converts the Keras model to TensorFlow Lite\n",
    "converter = lite.TocoConverter.from_keras_model_file(input_file)\n",
    "converter.post_training_quantize = True\n",
    "tflite_model = converter.convert()\n",
    "open(output_file, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac45f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
